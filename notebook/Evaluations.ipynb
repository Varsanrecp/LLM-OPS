{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7ff6fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello \n"
     ]
    }
   ],
   "source": [
    "print(\"Hello \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29c02638",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96801baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# QA\n",
    "inputs = [\n",
    "    \"For customer-facing applications, which company's models dominate the top rankings?\",\n",
    "    \"What percentage of respondents are using RAG in some form?\",\n",
    "    \"How often are most respondents updating their models?\",\n",
    "]\n",
    "\n",
    "outputs = [\n",
    "    \"OpenAI models dominate, with 3 of the top 5 and half of the top 10 most popular models for customer-facing apps.\",\n",
    "    \"70% of respondents are using RAG in some form.\",\n",
    "    \"More than 50% update their models at least monthly, with 17% doing so weekly.\",\n",
    "]\n",
    "\n",
    "# Dataset\n",
    "qa_pairs = [{\"question\": q, \"answer\": a} for q, a in zip(inputs, outputs)]\n",
    "df = pd.DataFrame(qa_pairs)\n",
    "\n",
    "# Write to csv\n",
    "csv_path = \"/Users/Hp/OneDrive/Desktop/LLM_OPS/data/goldens.csv\"\n",
    "df.to_csv(csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bd58e73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'example_ids': ['4c0b78dd-b68e-44e3-aa17-2196099ca676',\n",
       "  '70a138c3-1476-460f-a919-9d865431bd68',\n",
       "  '386753ef-0d4b-432f-8ab2-cefc7d0bf5d0'],\n",
       " 'count': 3}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "dataset_name = \"AgenticAIReportGoldens\"\n",
    "\n",
    "# Store\n",
    "dataset = client.create_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    description=\"Input and expected output pairs for AgenticAIReport\",\n",
    ")\n",
    "client.create_examples(\n",
    "    inputs=[{\"question\": q} for q in inputs],\n",
    "    outputs=[{\"answer\": a} for a in outputs],\n",
    "    dataset_id=dataset.id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "896fe341",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/Users/Hp/OneDrive/Desktop/LLM_OPS\")\n",
    "\n",
    "from pathlib import Path\n",
    "from multi_doc_chat.src.document_ingestion.data_ingestion import ChatIngestor\n",
    "from multi_doc_chat.src.document_chat.retrieval import ConversationalRAG\n",
    "import os\n",
    "\n",
    "# Simple file adapter for local file paths\n",
    "class LocalFileAdapter:\n",
    "    \"\"\"Adapter for local file paths to work with ChatIngestor.\"\"\"\n",
    "    def __init__(self, file_path: str):\n",
    "        self.path = Path(file_path)\n",
    "        self.name = self.path.name\n",
    "    \n",
    "    def getbuffer(self) -> bytes:\n",
    "        return self.path.read_bytes()\n",
    "\n",
    "\n",
    "def answer_ai_report_question(\n",
    "    inputs: dict,\n",
    "    data_path: str = \"/Users/Hp/OneDrive/Desktop/LLM_OPS/data/The 2025 AI Engineering Report.txt\",\n",
    "    chunk_size: int = 1000,\n",
    "    chunk_overlap: int = 200,\n",
    "    k: int = 5\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Answer questions about the AI Engineering Report using RAG.\n",
    "    \n",
    "    Args:\n",
    "        inputs: Dictionary containing the question, e.g., {\"question\": \"What is RAG?\"}\n",
    "        data_path: Path to the AI Engineering Report text file\n",
    "        chunk_size: Size of text chunks for splitting\n",
    "        chunk_overlap: Overlap between chunks\n",
    "        k: Number of documents to retrieve\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with the answer, e.g., {\"answer\": \"RAG stands for...\"}\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Extract question from inputs\n",
    "        question = inputs.get(\"question\", \"\")\n",
    "        if not question:\n",
    "            return {\"answer\": \"No question provided\"}\n",
    "        \n",
    "        # Check if file exists\n",
    "        if not Path(data_path).exists():\n",
    "            return {\"answer\": f\"Data file not found: {data_path}\"}\n",
    "        \n",
    "        # Create file adapter\n",
    "        file_adapter = LocalFileAdapter(data_path)\n",
    "        \n",
    "        # Build index using ChatIngestor\n",
    "        ingestor = ChatIngestor(\n",
    "            temp_base=\"data\",\n",
    "            faiss_base=\"faiss_index\",\n",
    "            use_session_dirs=True\n",
    "        )\n",
    "        \n",
    "        # Build retriever\n",
    "        ingestor.built_retriver(\n",
    "            uploaded_files=[file_adapter],\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            k=k\n",
    "        )\n",
    "        \n",
    "        # Get session ID and index path\n",
    "        session_id = ingestor.session_id\n",
    "        index_path = f\"faiss_index/{session_id}\"\n",
    "        \n",
    "        # Create RAG instance and load retriever\n",
    "        rag = ConversationalRAG(session_id=session_id)\n",
    "        rag.load_retriever_from_faiss(\n",
    "            index_path=index_path,\n",
    "            k=k,\n",
    "            index_name=os.getenv(\"FAISS_INDEX_NAME\", \"index\")\n",
    "        )\n",
    "        \n",
    "        # Get answer\n",
    "        answer = rag.invoke(question, chat_history=[])\n",
    "        \n",
    "        return {\"answer\": answer}\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\"answer\": f\"Error: {str(e)}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f57beb8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"timestamp\": \"2025-10-03T14:40:20.893780Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2025-10-03T14:40:20.898982Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2025-10-03T14:40:20.903462Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_eo...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2025-10-03T14:40:20.906511Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2025-10-03T14:40:20.922512Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"session_id\": \"session_20251003_201020_12e63841\", \"temp_dir\": \"data\\\\session_20251003_201020_12e63841\", \"faiss_dir\": \"faiss_index\\\\session_20251003_201020_12e63841\", \"sessionized\": true, \"timestamp\": \"2025-10-03T14:40:20.929194Z\", \"level\": \"info\", \"event\": \"ChatIngestor initialized\"}\n",
      "{\"uploaded\": \"The 2025 AI Engineering Report.txt\", \"saved_as\": \"data\\\\session_20251003_201020_12e63841\\\\d4394bd1.txt\", \"timestamp\": \"2025-10-03T14:40:21.002206Z\", \"level\": \"info\", \"event\": \"File saved for ingestion\"}\n",
      "{\"count\": 1, \"timestamp\": \"2025-10-03T14:40:21.033795Z\", \"level\": \"info\", \"event\": \"Documents loaded\"}\n",
      "{\"chunks\": 12, \"chunk_size\": 1000, \"overlap\": 200, \"timestamp\": \"2025-10-03T14:40:21.038545Z\", \"level\": \"info\", \"event\": \"Documents split\"}\n",
      "{\"model\": \"models/text-embedding-004\", \"timestamp\": \"2025-10-03T14:40:21.041787Z\", \"level\": \"info\", \"event\": \"Loading embedding model\"}\n",
      "Loading faiss with AVX2 support.\n",
      "Successfully loaded faiss with AVX2 support.\n",
      "{\"added\": 1, \"index\": \"faiss_index\\\\session_20251003_201020_12e63841\", \"timestamp\": \"2025-10-03T14:40:25.034645Z\", \"level\": \"info\", \"event\": \"FAISS index updated\"}\n",
      "{\"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.5, \"timestamp\": \"2025-10-03T14:40:25.047917Z\", \"level\": \"info\", \"event\": \"Using MMR search\"}\n",
      "{\"timestamp\": \"2025-10-03T14:40:25.072905Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2025-10-03T14:40:25.085841Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2025-10-03T14:40:25.102461Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_eo...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2025-10-03T14:40:25.108225Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2025-10-03T14:40:25.126239Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"provider\": \"google\", \"model\": \"gemini-2.5-pro\", \"timestamp\": \"2025-10-03T14:40:25.277054Z\", \"level\": \"info\", \"event\": \"Loading LLM\"}\n",
      "{\"session_id\": \"session_20251003_201020_12e63841\", \"timestamp\": \"2025-10-03T14:40:26.189299Z\", \"level\": \"info\", \"event\": \"LLM loaded successfully\"}\n",
      "{\"session_id\": \"session_20251003_201020_12e63841\", \"timestamp\": \"2025-10-03T14:40:26.194430Z\", \"level\": \"info\", \"event\": \"ConversationalRAG initialized\"}\n",
      "{\"timestamp\": \"2025-10-03T14:40:26.205235Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2025-10-03T14:40:26.213822Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2025-10-03T14:40:26.230992Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_eo...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2025-10-03T14:40:26.262075Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2025-10-03T14:40:26.366632Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"model\": \"models/text-embedding-004\", \"timestamp\": \"2025-10-03T14:40:26.369878Z\", \"level\": \"info\", \"event\": \"Loading embedding model\"}\n",
      "{\"session_id\": \"session_20251003_201020_12e63841\", \"timestamp\": \"2025-10-03T14:40:26.589240Z\", \"level\": \"info\", \"event\": \"LCEL graph built successfully\"}\n",
      "{\"index_path\": \"faiss_index/session_20251003_201020_12e63841\", \"index_name\": \"index\", \"search_type\": \"mmr\", \"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.5, \"session_id\": \"session_20251003_201020_12e63841\", \"timestamp\": \"2025-10-03T14:40:26.592377Z\", \"level\": \"info\", \"event\": \"FAISS retriever loaded successfully\"}\n",
      "{\"session_id\": \"session_20251003_201020_12e63841\", \"user_input\": \"For customer-facing applications, which company's models dominate the top rankings?\", \"answer_preview\": \"For customer-facing applications, OpenAI models are dominating the field. Three out of the top five most popular models for these applications are fro\", \"timestamp\": \"2025-10-03T14:40:38.028401Z\", \"level\": \"info\", \"event\": \"Chain invoked successfully\"}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: For customer-facing applications, which company's models dominate the top rankings?\n",
      "\n",
      "Answer: For customer-facing applications, OpenAI models are dominating the field. Three out of the top five most popular models for these applications are from OpenAI, as well as half of the top ten.\n"
     ]
    }
   ],
   "source": [
    "# Test the function with a sample question\n",
    "test_input = {\"question\": \"For customer-facing applications, which company's models dominate the top rankings?\"}\n",
    "result = answer_ai_report_question(test_input)\n",
    "print(\"Question:\", test_input[\"question\"])\n",
    "print(\"\\nAnswer:\", result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0a16ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.evaluation import evaluate, LangChainStringEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1ddd69a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing all questions from the dataset:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"timestamp\": \"2025-10-03T14:41:04.609813Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2025-10-03T14:41:04.614348Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2025-10-03T14:41:04.617924Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_eo...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2025-10-03T14:41:04.620642Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2025-10-03T14:41:04.629842Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"session_id\": \"session_20251003_201104_76c8bb44\", \"temp_dir\": \"data\\\\session_20251003_201104_76c8bb44\", \"faiss_dir\": \"faiss_index\\\\session_20251003_201104_76c8bb44\", \"sessionized\": true, \"timestamp\": \"2025-10-03T14:41:04.639088Z\", \"level\": \"info\", \"event\": \"ChatIngestor initialized\"}\n",
      "{\"uploaded\": \"The 2025 AI Engineering Report.txt\", \"saved_as\": \"data\\\\session_20251003_201104_76c8bb44\\\\73c61c5c.txt\", \"timestamp\": \"2025-10-03T14:41:04.668346Z\", \"level\": \"info\", \"event\": \"File saved for ingestion\"}\n",
      "{\"count\": 1, \"timestamp\": \"2025-10-03T14:41:04.708104Z\", \"level\": \"info\", \"event\": \"Documents loaded\"}\n",
      "{\"chunks\": 12, \"chunk_size\": 1000, \"overlap\": 200, \"timestamp\": \"2025-10-03T14:41:04.712005Z\", \"level\": \"info\", \"event\": \"Documents split\"}\n",
      "{\"model\": \"models/text-embedding-004\", \"timestamp\": \"2025-10-03T14:41:04.718485Z\", \"level\": \"info\", \"event\": \"Loading embedding model\"}\n",
      "{\"added\": 1, \"index\": \"faiss_index\\\\session_20251003_201104_76c8bb44\", \"timestamp\": \"2025-10-03T14:41:06.112825Z\", \"level\": \"info\", \"event\": \"FAISS index updated\"}\n",
      "{\"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.5, \"timestamp\": \"2025-10-03T14:41:06.115783Z\", \"level\": \"info\", \"event\": \"Using MMR search\"}\n",
      "{\"timestamp\": \"2025-10-03T14:41:06.125690Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2025-10-03T14:41:06.128896Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2025-10-03T14:41:06.132428Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_eo...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2025-10-03T14:41:06.136426Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2025-10-03T14:41:06.149747Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"provider\": \"google\", \"model\": \"gemini-2.5-pro\", \"timestamp\": \"2025-10-03T14:41:06.155875Z\", \"level\": \"info\", \"event\": \"Loading LLM\"}\n",
      "{\"session_id\": \"session_20251003_201104_76c8bb44\", \"timestamp\": \"2025-10-03T14:41:06.178957Z\", \"level\": \"info\", \"event\": \"LLM loaded successfully\"}\n",
      "{\"session_id\": \"session_20251003_201104_76c8bb44\", \"timestamp\": \"2025-10-03T14:41:06.184839Z\", \"level\": \"info\", \"event\": \"ConversationalRAG initialized\"}\n",
      "{\"timestamp\": \"2025-10-03T14:41:06.193169Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2025-10-03T14:41:06.195528Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2025-10-03T14:41:06.202817Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_eo...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2025-10-03T14:41:06.216965Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2025-10-03T14:41:06.234283Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"model\": \"models/text-embedding-004\", \"timestamp\": \"2025-10-03T14:41:06.237669Z\", \"level\": \"info\", \"event\": \"Loading embedding model\"}\n",
      "{\"session_id\": \"session_20251003_201104_76c8bb44\", \"timestamp\": \"2025-10-03T14:41:06.521755Z\", \"level\": \"info\", \"event\": \"LCEL graph built successfully\"}\n",
      "{\"index_path\": \"faiss_index/session_20251003_201104_76c8bb44\", \"index_name\": \"index\", \"search_type\": \"mmr\", \"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.5, \"session_id\": \"session_20251003_201104_76c8bb44\", \"timestamp\": \"2025-10-03T14:41:06.528480Z\", \"level\": \"info\", \"event\": \"FAISS retriever loaded successfully\"}\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 2\n",
      "Please retry in 48.53189208s. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-pro\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 2\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 48\n",
      "}\n",
      "].\n",
      "{\"error\": \"429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 2\\nPlease retry in 46.255534778s. [violations {\\n  quota_metric: \\\"generativelanguage.googleapis.com/generate_content_free_tier_requests\\\"\\n  quota_id: \\\"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\\\"\\n  quota_dimensions {\\n    key: \\\"model\\\"\\n    value: \\\"gemini-2.5-pro\\\"\\n  }\\n  quota_dimensions {\\n    key: \\\"location\\\"\\n    value: \\\"global\\\"\\n  }\\n  quota_value: 2\\n}\\n, links {\\n  description: \\\"Learn more about Gemini API quotas\\\"\\n  url: \\\"https://ai.google.dev/gemini-api/docs/rate-limits\\\"\\n}\\n, retry_delay {\\n  seconds: 46\\n}\\n]\", \"timestamp\": \"2025-10-03T14:41:13.922094Z\", \"level\": \"error\", \"event\": \"Failed to invoke ConversationalRAG\"}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1: For customer-facing applications, which company's models dominate the top rankings?\n",
      "A1: Error: Error in [c:\\Users\\Hp\\OneDrive\\Desktop\\LLM_OPS\\.venv\\Lib\\site-packages\\google\\api_core\\grpc_helpers.py] at line [78] | Message: Invocation error in ConversationalRAG\n",
      "Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users/Hp/OneDrive/Desktop/LLM_OPS\\multi_doc_chat\\src\\document_chat\\retrieval.py\", line 125, in invoke\n",
      "    answer = self.chain.invoke(payload)\n",
      "  File \"c:\\Users\\Hp\\OneDrive\\Desktop\\LLM_OPS\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py\", line 3046, in invoke\n",
      "    input_ = context.run(step.invoke, input_, config)\n",
      "  File \"c:\\Users\\Hp\\OneDrive\\Desktop\\LLM_OPS\\.venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py\", line 1334, in invoke\n",
      "    return super().invoke(input, config, stop=stop, **kwargs)\n",
      "           ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Hp\\OneDrive\\Desktop\\LLM_OPS\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 395, in invoke\n",
      "    self.generate_prompt(\n",
      "    ~~~~~~~~~~~~~~~~~~~~^\n",
      "        [self._convert_input(input)],\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<6 lines>...\n",
      "        **kwargs,\n",
      "        ^^^^^^^^^\n",
      "    ).generations[0][0],\n",
      "    ^\n",
      "  File \"c:\\Users\\Hp\\OneDrive\\Desktop\\LLM_OPS\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 980, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "           ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Hp\\OneDrive\\Desktop\\LLM_OPS\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 799, in generate\n",
      "    self._generate_with_cache(\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        m,\n",
      "        ^^\n",
      "    ...<2 lines>...\n",
      "        **kwargs,\n",
      "        ^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"c:\\Users\\Hp\\OneDrive\\Desktop\\LLM_OPS\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 1045, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "        messages, stop=stop, run_manager=run_manager, **kwargs\n",
      "    )\n",
      "  File \"c:\\Users\\Hp\\OneDrive\\Desktop\\LLM_OPS\\.venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py\", line 1441, in _generate\n",
      "    response: GenerateContentResponse = _chat_with_retry(\n",
      "                                        ~~~~~~~~~~~~~~~~^\n",
      "        request=request,\n",
      "        ^^^^^^^^^^^^^^^^\n",
      "    ...<2 lines>...\n",
      "        metadata=self.default_metadata,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"c:\\Users\\Hp\\OneDrive\\Desktop\\LLM_OPS\\.venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py\", line 231, in _chat_with_retry\n",
      "    return _chat_with_retry(**params)\n",
      "  File \"c:\\Users\\Hp\\OneDrive\\Desktop\\LLM_OPS\\.venv\\Lib\\site-packages\\tenacity\\__init__.py\", line 338, in wrapped_f\n",
      "    return copy(f, *args, **kw)\n",
      "  File \"c:\\Users\\Hp\\OneDrive\\Desktop\\LLM_OPS\\.venv\\Lib\\site-packages\\tenacity\\__init__.py\", line 477, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"c:\\Users\\Hp\\OneDrive\\Desktop\\LLM_OPS\\.venv\\Lib\\site-packages\\tenacity\\__init__.py\", line 378, in iter\n",
      "    result = action(retry_state)\n",
      "  File \"c:\\Users\\Hp\\OneDrive\\Desktop\\LLM_OPS\\.venv\\Lib\\site-packages\\tenacity\\__init__.py\", line 420, in exc_check\n",
      "    raise retry_exc.reraise()\n",
      "          ~~~~~~~~~~~~~~~~~^^\n",
      "  File \"c:\\Users\\Hp\\OneDrive\\Desktop\\LLM_OPS\\.venv\\Lib\\site-packages\\tenacity\\__init__.py\", line 187, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "          ~~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"C:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\concurrent\\futures\\_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ~~~~~~~~~~~~~~~~~^^\n",
      "  File \"C:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\concurrent\\futures\\_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"c:\\Users\\Hp\\OneDrive\\Desktop\\LLM_OPS\\.venv\\Lib\\site-packages\\tenacity\\__init__.py\", line 480, in __call__\n",
      "    result = fn(*args, **kwargs)\n",
      "  File \"c:\\Users\\Hp\\OneDrive\\Desktop\\LLM_OPS\\.venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py\", line 222, in _chat_with_retry\n",
      "    raise e\n",
      "  File \"c:\\Users\\Hp\\OneDrive\\Desktop\\LLM_OPS\\.venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py\", line 206, in _chat_with_retry\n",
      "    return generation_method(**kwargs)\n",
      "  File \"c:\\Users\\Hp\\OneDrive\\Desktop\\LLM_OPS\\.venv\\Lib\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\generative_service\\client.py\", line 868, in generate_content\n",
      "    response = rpc(\n",
      "        request,\n",
      "    ...<2 lines>...\n",
      "        metadata=metadata,\n",
      "    )\n",
      "  File \"c:\\Users\\Hp\\OneDrive\\Desktop\\LLM_OPS\\.venv\\Lib\\site-packages\\google\\api_core\\gapic_v1\\method.py\", line 131, in __call__\n",
      "    return wrapped_func(*args, **kwargs)\n",
      "  File \"c:\\Users\\Hp\\OneDrive\\Desktop\\LLM_OPS\\.venv\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py\", line 294, in retry_wrapped_func\n",
      "    return retry_target(\n",
      "        target,\n",
      "    ...<3 lines>...\n",
      "        on_error=on_error,\n",
      "    )\n",
      "  File \"c:\\Users\\Hp\\OneDrive\\Desktop\\LLM_OPS\\.venv\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py\", line 156, in retry_target\n",
      "    next_sleep = _retry_error_helper(\n",
      "        exc,\n",
      "    ...<6 lines>...\n",
      "        timeout,\n",
      "    )\n",
      "  File \"c:\\Users\\Hp\\OneDrive\\Desktop\\LLM_OPS\\.venv\\Lib\\site-packages\\google\\api_core\\retry\\retry_base.py\", line 214, in _retry_error_helper\n",
      "    raise final_exc from source_exc\n",
      "  File \"c:\\Users\\Hp\\OneDrive\\Desktop\\LLM_OPS\\.venv\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py\", line 147, in retry_target\n",
      "    result = target()\n",
      "  File \"c:\\Users\\Hp\\OneDrive\\Desktop\\LLM_OPS\\.venv\\Lib\\site-packages\\google\\api_core\\timeout.py\", line 130, in func_with_timeout\n",
      "    return func(*args, **kwargs)\n",
      "  File \"c:\\Users\\Hp\\OneDrive\\Desktop\\LLM_OPS\\.venv\\Lib\\site-packages\\google\\api_core\\grpc_helpers.py\", line 78, in error_remapped_callable\n",
      "    raise exceptions.from_grpc_error(exc) from exc\n",
      "google.api_core.exceptions.ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 2\n",
      "Please retry in 46.255534778s. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-pro\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 2\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 46\n",
      "}\n",
      "]\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"timestamp\": \"2025-10-03T14:41:13.964439Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2025-10-03T14:41:13.967238Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2025-10-03T14:41:13.970053Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_eo...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2025-10-03T14:41:13.972546Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2025-10-03T14:41:13.984225Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"session_id\": \"session_20251003_201113_a082944e\", \"temp_dir\": \"data\\\\session_20251003_201113_a082944e\", \"faiss_dir\": \"faiss_index\\\\session_20251003_201113_a082944e\", \"sessionized\": true, \"timestamp\": \"2025-10-03T14:41:13.994822Z\", \"level\": \"info\", \"event\": \"ChatIngestor initialized\"}\n",
      "{\"uploaded\": \"The 2025 AI Engineering Report.txt\", \"saved_as\": \"data\\\\session_20251003_201113_a082944e\\\\a4d7e5f9.txt\", \"timestamp\": \"2025-10-03T14:41:14.000755Z\", \"level\": \"info\", \"event\": \"File saved for ingestion\"}\n",
      "{\"count\": 1, \"timestamp\": \"2025-10-03T14:41:14.035630Z\", \"level\": \"info\", \"event\": \"Documents loaded\"}\n",
      "{\"chunks\": 12, \"chunk_size\": 1000, \"overlap\": 200, \"timestamp\": \"2025-10-03T14:41:14.041505Z\", \"level\": \"info\", \"event\": \"Documents split\"}\n",
      "{\"model\": \"models/text-embedding-004\", \"timestamp\": \"2025-10-03T14:41:14.044527Z\", \"level\": \"info\", \"event\": \"Loading embedding model\"}\n",
      "{\"added\": 1, \"index\": \"faiss_index\\\\session_20251003_201113_a082944e\", \"timestamp\": \"2025-10-03T14:41:15.658280Z\", \"level\": \"info\", \"event\": \"FAISS index updated\"}\n",
      "{\"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.5, \"timestamp\": \"2025-10-03T14:41:15.674478Z\", \"level\": \"info\", \"event\": \"Using MMR search\"}\n",
      "{\"timestamp\": \"2025-10-03T14:41:15.686644Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2025-10-03T14:41:15.689839Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2025-10-03T14:41:15.693408Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_eo...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2025-10-03T14:41:15.696018Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2025-10-03T14:41:15.710264Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"provider\": \"google\", \"model\": \"gemini-2.5-pro\", \"timestamp\": \"2025-10-03T14:41:15.716002Z\", \"level\": \"info\", \"event\": \"Loading LLM\"}\n",
      "{\"session_id\": \"session_20251003_201113_a082944e\", \"timestamp\": \"2025-10-03T14:41:15.731153Z\", \"level\": \"info\", \"event\": \"LLM loaded successfully\"}\n",
      "{\"session_id\": \"session_20251003_201113_a082944e\", \"timestamp\": \"2025-10-03T14:41:15.735029Z\", \"level\": \"info\", \"event\": \"ConversationalRAG initialized\"}\n",
      "{\"timestamp\": \"2025-10-03T14:41:15.746276Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2025-10-03T14:41:15.749307Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2025-10-03T14:41:15.754767Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_eo...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2025-10-03T14:41:15.761145Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2025-10-03T14:41:15.772252Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"model\": \"models/text-embedding-004\", \"timestamp\": \"2025-10-03T14:41:15.775543Z\", \"level\": \"info\", \"event\": \"Loading embedding model\"}\n",
      "{\"session_id\": \"session_20251003_201113_a082944e\", \"timestamp\": \"2025-10-03T14:41:15.875454Z\", \"level\": \"info\", \"event\": \"LCEL graph built successfully\"}\n",
      "{\"index_path\": \"faiss_index/session_20251003_201113_a082944e\", \"index_name\": \"index\", \"search_type\": \"mmr\", \"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.5, \"session_id\": \"session_20251003_201113_a082944e\", \"timestamp\": \"2025-10-03T14:41:15.878454Z\", \"level\": \"info\", \"event\": \"FAISS retriever loaded successfully\"}\n",
      "{\"session_id\": \"session_20251003_201113_a082944e\", \"user_input\": \"What percentage of respondents are using RAG in some form?\", \"answer_preview\": \"According to the provided text, 70% of respondents say they are using RAG (retrieval augmented generation) in one way or another.\", \"timestamp\": \"2025-10-03T14:41:42.265255Z\", \"level\": \"info\", \"event\": \"Chain invoked successfully\"}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q2: What percentage of respondents are using RAG in some form?\n",
      "A2: According to the provided text, 70% of respondents say they are using RAG (retrieval augmented generation) in one way or another.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"timestamp\": \"2025-10-03T14:41:42.290840Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2025-10-03T14:41:42.294494Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2025-10-03T14:41:42.298422Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_eo...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2025-10-03T14:41:42.302403Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2025-10-03T14:41:42.316944Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"session_id\": \"session_20251003_201142_8b89d960\", \"temp_dir\": \"data\\\\session_20251003_201142_8b89d960\", \"faiss_dir\": \"faiss_index\\\\session_20251003_201142_8b89d960\", \"sessionized\": true, \"timestamp\": \"2025-10-03T14:41:42.324620Z\", \"level\": \"info\", \"event\": \"ChatIngestor initialized\"}\n",
      "{\"uploaded\": \"The 2025 AI Engineering Report.txt\", \"saved_as\": \"data\\\\session_20251003_201142_8b89d960\\\\d0796874.txt\", \"timestamp\": \"2025-10-03T14:41:42.371751Z\", \"level\": \"info\", \"event\": \"File saved for ingestion\"}\n",
      "{\"count\": 1, \"timestamp\": \"2025-10-03T14:41:42.415221Z\", \"level\": \"info\", \"event\": \"Documents loaded\"}\n",
      "{\"chunks\": 12, \"chunk_size\": 1000, \"overlap\": 200, \"timestamp\": \"2025-10-03T14:41:42.419920Z\", \"level\": \"info\", \"event\": \"Documents split\"}\n",
      "{\"model\": \"models/text-embedding-004\", \"timestamp\": \"2025-10-03T14:41:42.425115Z\", \"level\": \"info\", \"event\": \"Loading embedding model\"}\n",
      "{\"added\": 1, \"index\": \"faiss_index\\\\session_20251003_201142_8b89d960\", \"timestamp\": \"2025-10-03T14:41:44.144773Z\", \"level\": \"info\", \"event\": \"FAISS index updated\"}\n",
      "{\"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.5, \"timestamp\": \"2025-10-03T14:41:44.147856Z\", \"level\": \"info\", \"event\": \"Using MMR search\"}\n",
      "{\"timestamp\": \"2025-10-03T14:41:44.154944Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2025-10-03T14:41:44.157864Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2025-10-03T14:41:44.160617Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_eo...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2025-10-03T14:41:44.165469Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2025-10-03T14:41:44.179356Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"provider\": \"google\", \"model\": \"gemini-2.5-pro\", \"timestamp\": \"2025-10-03T14:41:44.183049Z\", \"level\": \"info\", \"event\": \"Loading LLM\"}\n",
      "{\"session_id\": \"session_20251003_201142_8b89d960\", \"timestamp\": \"2025-10-03T14:41:44.194820Z\", \"level\": \"info\", \"event\": \"LLM loaded successfully\"}\n",
      "{\"session_id\": \"session_20251003_201142_8b89d960\", \"timestamp\": \"2025-10-03T14:41:44.200827Z\", \"level\": \"info\", \"event\": \"ConversationalRAG initialized\"}\n",
      "{\"timestamp\": \"2025-10-03T14:41:44.208757Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2025-10-03T14:41:44.210708Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2025-10-03T14:41:44.214080Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_eo...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2025-10-03T14:41:44.217915Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2025-10-03T14:41:44.235907Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"model\": \"models/text-embedding-004\", \"timestamp\": \"2025-10-03T14:41:44.242606Z\", \"level\": \"info\", \"event\": \"Loading embedding model\"}\n",
      "{\"session_id\": \"session_20251003_201142_8b89d960\", \"timestamp\": \"2025-10-03T14:41:44.330183Z\", \"level\": \"info\", \"event\": \"LCEL graph built successfully\"}\n",
      "{\"index_path\": \"faiss_index/session_20251003_201142_8b89d960\", \"index_name\": \"index\", \"search_type\": \"mmr\", \"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.5, \"session_id\": \"session_20251003_201142_8b89d960\", \"timestamp\": \"2025-10-03T14:41:44.333088Z\", \"level\": \"info\", \"event\": \"FAISS retriever loaded successfully\"}\n",
      "{\"session_id\": \"session_20251003_201142_8b89d960\", \"user_input\": \"How often are most respondents updating their models?\", \"answer_preview\": \"More than 50% of respondents are updating their models at least monthly. Of those, 17% are updating their models on a weekly basis.\", \"timestamp\": \"2025-10-03T14:42:10.347788Z\", \"level\": \"info\", \"event\": \"Chain invoked successfully\"}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q3: How often are most respondents updating their models?\n",
      "A3: More than 50% of respondents are updating their models at least monthly. Of those, 17% are updating their models on a weekly basis.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example: Test with all golden questions\n",
    "print(\"Testing all questions from the dataset:\\n\")\n",
    "for i, q in enumerate(inputs, 1):\n",
    "    test_input = {\"question\": q}\n",
    "    result = answer_ai_report_question(test_input)\n",
    "    print(f\"Q{i}: {q}\")\n",
    "    print(f\"A{i}: {result['answer']}\\n\")\n",
    "    print(\"-\" * 80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e6162bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def answer_ai_report_question(example: dict) -> str:\n",
      "    print(\"DEBUG INPUT:\", example)  # 👈 see what’s being passed in\n",
      "\n",
      "    question = example.get(\"question\") or example.get(\"input\") or example[\"prompt\"]\n",
      "    context = example.get(\"context\", \"\")\n",
      "\n",
      "    print(\"DEBUG QUESTION:\", question)\n",
      "    print(\"DEBUG CONTEXT:\", context[:200], \"...\")  # limit long text\n",
      "\n",
      "    response = gemini_llm.invoke(\n",
      "        f\"Answer based on context:\\n{context}\\n\\nQ: {question}\"\n",
      "    )\n",
      "\n",
      "    print(\"DEBUG RESPONSE:\", response)\n",
      "    return response if isinstance(response, str) else str(response)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "\n",
    "print(inspect.getsource(answer_ai_report_question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5edccbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'test-AgenticAIReportGoldens-qa-rag-7667671b' at:\n",
      "https://smith.langchain.com/o/c52f60ad-54d1-4235-9051-676106d86b53/datasets/ef062c10-af21-4b1c-bafb-0b332d90dd4d/compare?selectedSessions=bc26fc1a-c9e7-4583-9d0e-f03fd537c969\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:37, 12.61s/it]\n"
     ]
    }
   ],
   "source": [
    "from langsmith.evaluation import evaluate, LangChainStringEvaluator\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI # 1. Import Gemini LLM\n",
    "\n",
    "# --- Configuration ---\n",
    "# 2. Instantiate the Gemini LLM for the evaluator\n",
    "# Note: Ensure you have your GEMINI_API_KEY environment variable set.\n",
    "eval_llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\") \n",
    "# 3. Pass the Gemini LLM to the cot_qa evaluator via the 'config' dictionary\n",
    "qa_evaluator = [\n",
    "    LangChainStringEvaluator(\n",
    "        \"cot_qa\", \n",
    "        config={\"llm\": eval_llm} \n",
    "    )\n",
    "]\n",
    "\n",
    "dataset_name = \"AgenticAIReportGoldens\"\n",
    "# Assuming 'answer_ai_report_question' is a function defined elsewhere that takes inputs \n",
    "# from the dataset and returns a prediction (e.g., your RAG chain).\n",
    "\n",
    "# Run evaluation using your RAG function\n",
    "experiment_results = evaluate(\n",
    "    answer_ai_report_question,\n",
    "    data=dataset_name,\n",
    "    evaluators=qa_evaluator,\n",
    "    experiment_prefix=\"test-AgenticAIReportGoldens-qa-rag\",\n",
    "    # Experiment metadata\n",
    "    metadata={\n",
    "        \"variant\": \"RAG with FAISS and AI Engineering Report\",\n",
    "        \"chunk_size\": 1000,\n",
    "        \"chunk_overlap\": 200,\n",
    "        \"k\": 5,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1bb56aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.schemas import Run, Example\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "def correctness_evaluator(run: Run, example: Example) -> dict:\n",
    "    \"\"\"\n",
    "    Custom LLM-as-a-Judge evaluator for correctness.\n",
    "    \n",
    "    Correctness means how well the actual model output matches the reference output \n",
    "    in terms of factual accuracy, coverage, and meaning.\n",
    "    \n",
    "    Args:\n",
    "        run: The Run object containing the actual outputs\n",
    "        example: The Example object containing the expected outputs\n",
    "    \n",
    "    Returns:\n",
    "        dict with 'score' (1 for correct, 0 for incorrect) and 'reasoning'\n",
    "    \"\"\"\n",
    "    # Extract actual and expected outputs\n",
    "    actual_output = run.outputs.get(\"answer\", \"\")\n",
    "    expected_output = example.outputs.get(\"answer\", \"\")\n",
    "    input_question = example.inputs.get(\"question\", \"\")\n",
    "    \n",
    "    # Define the evaluation prompt\n",
    "    eval_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"You are an evaluator whose job is to judge correctness.\n",
    "\n",
    "Correctness means how well the actual model output matches the reference output in terms of factual accuracy, coverage, and meaning.\n",
    "\n",
    "- If the actual output matches the reference output semantically (even if wording differs), it should be marked correct.\n",
    "- If the output misses key facts, introduces contradictions, or is factually incorrect, it should be marked incorrect.\n",
    "\n",
    "Do not penalize for stylistic or formatting differences unless they change meaning.\"\"\"),\n",
    "        (\"human\", \"\"\"<example>\n",
    "<input>\n",
    "{input}\n",
    "</input>\n",
    "\n",
    "<output>\n",
    "Expected Output: {expected_output}\n",
    "\n",
    "Actual Output: {actual_output}\n",
    "</output>\n",
    "</example>\n",
    "\n",
    "Please grade the following agent run given the input, expected output, and actual output.\n",
    "Focus only on correctness (semantic and factual alignment).\n",
    "\n",
    "Respond with:\n",
    "1. A brief reasoning (1-2 sentences)\n",
    "2. A final verdict: either \"CORRECT\" or \"INCORRECT\"\n",
    "\n",
    "Format your response as:\n",
    "Reasoning: [your reasoning]\n",
    "Verdict: [CORRECT or INCORRECT]\"\"\")\n",
    "    ])\n",
    "    \n",
    "    # Initialize LLM (using Gemini as shown in your config)\n",
    "    llm = ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    # Create chain and invoke\n",
    "    chain = eval_prompt | llm\n",
    "    \n",
    "    try:\n",
    "        response = chain.invoke({\n",
    "            \"input\": input_question,\n",
    "            \"expected_output\": expected_output,\n",
    "            \"actual_output\": actual_output\n",
    "        })\n",
    "        \n",
    "        response_text = response.content\n",
    "        \n",
    "        # Parse the response\n",
    "        reasoning = \"\"\n",
    "        verdict = \"\"\n",
    "        \n",
    "        for line in response_text.split('\\n'):\n",
    "            if line.startswith(\"Reasoning:\"):\n",
    "                reasoning = line.replace(\"Reasoning:\", \"\").strip()\n",
    "            elif line.startswith(\"Verdict:\"):\n",
    "                verdict = line.replace(\"Verdict:\", \"\").strip()\n",
    "        \n",
    "        # Convert verdict to score (1 for correct, 0 for incorrect)\n",
    "        score = 1 if \"CORRECT\" in verdict.upper() else 0\n",
    "        \n",
    "        return {\n",
    "            \"key\": \"correctness\",\n",
    "            \"score\": score,\n",
    "            \"reasoning\": reasoning,\n",
    "            \"comment\": f\"Verdict: {verdict}\"\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"key\": \"correctness\",\n",
    "            \"score\": 0,\n",
    "            \"reasoning\": f\"Error during evaluation: {str(e)}\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5a84b364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'agenticAIReport-correctness-eval-8d421491' at:\n",
      "https://smith.langchain.com/o/c52f60ad-54d1-4235-9051-676106d86b53/datasets/ef062c10-af21-4b1c-bafb-0b332d90dd4d/compare?selectedSessions=a8fb2829-2688-4585-a0af-95080845a651\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:25,  8.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation completed! Check the LangSmith UI for detailed results.\n"
     ]
    }
   ],
   "source": [
    "from langsmith.evaluation import evaluate\n",
    "\n",
    "# Define evaluators - using custom correctness evaluator\n",
    "evaluators = [correctness_evaluator]\n",
    "\n",
    "dataset_name = \"AgenticAIReportGoldens\"\n",
    "\n",
    "# Run evaluation\n",
    "experiment_results = evaluate(\n",
    "    answer_ai_report_question,\n",
    "    data=dataset_name,\n",
    "    evaluators=evaluators,\n",
    "    experiment_prefix=\"agenticAIReport-correctness-eval\",\n",
    "    description=\"Evaluating RAG system with custom correctness evaluator (LLM-as-a-Judge)\",\n",
    "    metadata={\n",
    "        \"variant\": \"RAG with FAISS and AI Engineering Report\",\n",
    "        \"evaluator\": \"custom_correctness_llm_judge\",\n",
    "        \"model\": \"gemini-2.5-flash\",\n",
    "        \"chunk_size\": 1000,\n",
    "        \"chunk_overlap\": 200,\n",
    "        \"k\": 5,\n",
    "    },\n",
    ")\n",
    "\n",
    "print(\"\\nEvaluation completed! Check the LangSmith UI for detailed results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b965ea2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-ops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
